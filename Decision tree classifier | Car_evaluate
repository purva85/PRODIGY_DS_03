import pandas as pd
import numpy as np
import math

data = pd.read_csv('/content/car_evaluate.csv')
print(data)

# Calculate entropy for a given dataset
def entropy(target_col):
    entropy = 0
    values = target_col.unique()
    for value in values:
        p = (target_col == value).sum() / len(target_col)
        entropy += -p * math.log2(p)
    return entropy

# Calculate information gain
def information_gain(data, feature, target):
    total_entropy = entropy(data[target])
    values = data[feature].unique()
    weighted_entropy = 0
    for value in values:
        subset = data[data[feature] == value]
        weighted_entropy += (len(subset) / len(data)) * entropy(subset[target])
    return total_entropy - weighted_entropy

# Define a function to build the decision tree
def build_tree(data, target, features, level=0):
    # Base case: If all examples have the same target value, return that value
    if len(data[target].unique()) == 1:
        return data[target].unique()[0]

    # Base case: If there are no features left to split, return the majority target value
    if len(features) == 0:
        return data[target].mode()[0]

    # Find the best feature to split on based on information gain
    best_feature = max(features, key=lambda feature: information_gain(data, feature, target))

    # Calculate entropy before split
    total_entropy = entropy(data[target])

    # Print entropy calculation
    print("  " * level + f"Entropy({target}) = {total_entropy:.4f}")

    # Create the tree structure
    tree = {best_feature: {}}
    features.remove(best_feature)

    # Recursively build the tree for each branch
    for value in data[best_feature].unique():
        subset = data[data[best_feature] == value]
        subtree = build_tree(subset, target, features, level + 1)
        tree[best_feature][value] = subtree

    return tree

features = ['door', 'person', 'safety']

decision_tree = build_tree( data,'decision', features)

# Print the decision tree
import pprint
pprint.pprint(decision_tree)





